{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREPARACION DE DATOS PARA ENTRENAR MODELO**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/machinelearning.jpg\" alt=\"Texto alternativo\" width=\"2100\" height=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el data, anteriormente lo pasamos todo a numericas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resultados = pd.read_csv(\"../BASESDEDATOS/CSVs/LimpiezaEncoded_2PRUEBA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resultado\n",
       "0    0.631378\n",
       "1    0.368622\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resultados.Resultado.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Como vemos y como vimos en las gráficas en el EDA la distribución de nuestro target \"Resultado\" es desigual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vamos aplicar un SMOTE para igualar las clases**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/smote.jpg\" alt=\"Texto alternativo\" width=\"1800\" height=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_resultados.drop(columns=['Resultado'])\n",
    "y = data_resultados['Resultado']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Separación X_Train e Y_Train**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/xtrain.jpg\" alt=\"Texto alternativo\" width=\"1400\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE para balancear las clases\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Construcción de los modelos**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/construccion.jpg\" alt=\"Texto alternativo\" width=\"1600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 1: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Mejores parámetros: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       850\n",
      "           1       0.54      0.58      0.56       460\n",
      "\n",
      "    accuracy                           0.68      1310\n",
      "   macro avg       0.65      0.66      0.66      1310\n",
      "weighted avg       0.69      0.68      0.68      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[622 228]\n",
      " [191 269]]\n",
      "Accuracy: 0.6801526717557251\n",
      "Modelo guardado como best_log_model_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el modelo y los nuevos parámetros de GridSearch\n",
    "log_reg = LogisticRegression(max_iter=1000,class_weight='balanced', random_state=42)\n",
    "param_grid = [\n",
    "    {'C': [100], 'solver': ['liblinear'], 'penalty': ['l1']},\n",
    "    {'C': [100], 'solver': ['saga'], 'penalty': ['l1'], 'l1_ratio': [0, 0.5, 1]}\n",
    "]\n",
    "\n",
    "# Aplicar GridSearchCV\n",
    "grid_log_reg = GridSearchCV(log_reg, param_grid, cv=3, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_log_reg = grid_log_reg.best_estimator_\n",
    "y_pred_log_reg = best_log_reg.predict(X_test)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Mejores parámetros:\", grid_log_reg.best_params_)\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_log_model_3_PRUEBA.joblib'\n",
    "dump(best_log_reg, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 2: Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "Mejores parámetros: {'max_depth': 8, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.65      0.70       850\n",
      "           1       0.49      0.60      0.54       460\n",
      "\n",
      "    accuracy                           0.64      1310\n",
      "   macro avg       0.62      0.63      0.62      1310\n",
      "weighted avg       0.66      0.64      0.64      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[556 294]\n",
      " [182 278]]\n",
      "Accuracy: 0.6366412213740458\n",
      "Modelo guardado como best_tree_model_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Definir el modelo y los nuevos parámetros de GridSearch\n",
    "decision_tree = DecisionTreeClassifier(class_weight='balanced',random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 8, 10, 12, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [None, 'sqrt']\n",
    "}\n",
    "\n",
    "# Aplicar GridSearchCV\n",
    "grid_tree = GridSearchCV(decision_tree, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_tree = grid_tree.best_estimator_\n",
    "y_pred_tree = best_tree.predict(X_test)\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(\"Mejores parámetros:\", grid_tree.best_params_)\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_tree_model_3_PRUEBA.joblib'\n",
    "dump(best_tree, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 3: Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       850\n",
      "           1       0.57      0.51      0.54       460\n",
      "\n",
      "    accuracy                           0.69      1310\n",
      "   macro avg       0.66      0.65      0.65      1310\n",
      "weighted avg       0.69      0.69      0.69      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[671 179]\n",
      " [225 235]]\n",
      "Accuracy: 0.6916030534351145\n",
      "Modelo guardado como best_forest_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "random_forest = RandomForestClassifier(class_weight='balanced',random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(random_forest, param_distributions, n_iter=150, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_forest_random = random_search.best_estimator_\n",
    "y_pred_forest_random = best_forest_random.predict(X_test)\n",
    "print(\"Random Forest Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_forest_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_forest_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_forest_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_forest_model_random_3_PRUEBA.joblib'\n",
    "dump(best_forest_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 4: Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_depth': 11, 'learning_rate': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       850\n",
      "           1       0.57      0.51      0.54       460\n",
      "\n",
      "    accuracy                           0.69      1310\n",
      "   macro avg       0.66      0.65      0.66      1310\n",
      "weighted avg       0.69      0.69      0.69      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[676 174]\n",
      " [226 234]]\n",
      "Accuracy: 0.6946564885496184\n",
      "Modelo guardado como best_gb_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [400,500,600],\n",
    "    'learning_rate': [0.2, 0.1],\n",
    "    'max_depth': [7,9,11,13],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [6, 8,10,12],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_gb = RandomizedSearchCV(gradient_boosting, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_gb_random = random_search_gb.best_estimator_\n",
    "y_pred_gb_random = best_gb_random.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_gb.best_params_)\n",
    "print(classification_report(y_test, y_pred_gb_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_gb_model_random_3_PRUEBA.joblib'\n",
    "dump(best_gb_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 8: XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost con RandomizedSearchCV\n",
      "Mejores parámetros: {'subsample': 0.8, 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 13, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       850\n",
      "           1       0.58      0.53      0.55       460\n",
      "\n",
      "    accuracy                           0.70      1310\n",
      "   macro avg       0.67      0.66      0.66      1310\n",
      "weighted avg       0.70      0.70      0.70      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[674 176]\n",
      " [216 244]]\n",
      "Accuracy: 0.7007633587786259\n",
      "Modelo guardado como best_xgb_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss',scale_pos_weight=1)\n",
    "param_distributions = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [9, 11, 13, 15,17,19],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_xgb_random = random_search_xgb.best_estimator_\n",
    "y_pred_xgb_random = best_xgb_random.predict(X_test)\n",
    "print(\"XGBoost con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_xgb.best_params_)\n",
    "print(classification_report(y_test, y_pred_xgb_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_xgb_model_random_3_PRUEBA.joblib'\n",
    "dump(best_xgb_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 9: Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raul_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 36 is smaller than n_iter=100. Running 36 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 300, 'max_samples': 1.0, 'max_features': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       850\n",
      "           1       0.58      0.53      0.55       460\n",
      "\n",
      "    accuracy                           0.70      1310\n",
      "   macro avg       0.67      0.66      0.66      1310\n",
      "weighted avg       0.69      0.70      0.70      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[675 175]\n",
      " [218 242]]\n",
      "Accuracy: 0.7\n",
      "Modelo guardado como best_bagging_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier( random_state=42)\n",
    "\n",
    "# Definir los parámetros para RandomizedSearch\n",
    "param_distributions = {\n",
    "    'n_estimators': [200,300,400],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.4, 0.5, 0.7, 1.0],\n",
    "\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_bagging = RandomizedSearchCV(bagging_model, param_distributions, n_iter=100, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_bagging_random = random_search_bagging.best_estimator_\n",
    "y_pred_bagging_random = best_bagging_random.predict(X_test)\n",
    "print(\"Bagging Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_bagging.best_params_)\n",
    "print(classification_report(y_test, y_pred_bagging_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_bagging_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bagging_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_bagging_model_random_3_PRUEBA.joblib'\n",
    "dump(best_bagging_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 10: LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raul_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 24 is smaller than n_iter=100. Running 24 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3283, number of negative: 3283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5450\n",
      "[LightGBM] [Info] Number of data points in the train set: 6566, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'num_leaves': 90, 'n_estimators': 600, 'max_depth': 30, 'learning_rate': 0.3, 'boosting_type': 'dart'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       850\n",
      "           1       0.56      0.50      0.53       460\n",
      "\n",
      "    accuracy                           0.69      1310\n",
      "   macro avg       0.65      0.65      0.65      1310\n",
      "weighted avg       0.68      0.69      0.68      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[669 181]\n",
      " [228 232]]\n",
      "Accuracy: 0.6877862595419847\n",
      "Modelo guardado como best_lgbm_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "lgbm_model = LGBMClassifier(class_weight='balanced',random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [600],\n",
    "    'learning_rate': [0.2, 0.3],\n",
    "    'num_leaves': [70, 90],\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'max_depth': [20, 30 , 40]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm_model, param_distributions, n_iter=100, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "y_pred_lgbm = best_lgbm.predict(X_test)\n",
    "print(\"LightGBM Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_lgbm.best_params_)\n",
    "print(classification_report(y_test, y_pred_lgbm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lgbm))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_lgbm_model_random_3_PRUEBA.joblib'\n",
    "dump(best_lgbm, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 11: CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raul_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 1 is smaller than n_iter=50. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'learning_rate': 0.1, 'l2_leaf_reg': 3, 'iterations': 300, 'depth': 12, 'border_count': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.78       850\n",
      "           1       0.58      0.52      0.55       460\n",
      "\n",
      "    accuracy                           0.70      1310\n",
      "   macro avg       0.67      0.66      0.66      1310\n",
      "weighted avg       0.69      0.70      0.70      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[677 173]\n",
      " [220 240]]\n",
      "Accuracy: 0.7\n",
      "Modelo guardado como best_catboost_model_random_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Definir el modelo de CatBoost\n",
    "catboost_model = CatBoostClassifier(auto_class_weights='Balanced', random_state=42, silent=True)\n",
    "\n",
    "# Definir los parámetros para RandomizedSearch\n",
    "param_distributions = {\n",
    "    'iterations': [300],\n",
    "    'learning_rate': [0.1],\n",
    "    'depth': [12],\n",
    "    'l2_leaf_reg': [3],\n",
    "    'border_count': [100]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_catboost = RandomizedSearchCV(catboost_model, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_catboost_random = random_search_catboost.best_estimator_\n",
    "y_pred_catboost_random = best_catboost_random.predict(X_test)\n",
    "print(\"CatBoost Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_catboost.best_params_)\n",
    "print(classification_report(y_test, y_pred_catboost_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_catboost_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_catboost_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_catboost_model_random_3_PRUEBA.joblib'\n",
    "dump(best_catboost_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stacking de los modelos**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/stacking.jpg\" alt=\"Texto alternativo\" width=\"1200\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking con Meta modelo LogisticRegresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       850\n",
      "           1       0.57      0.52      0.54       460\n",
      "\n",
      "    accuracy                           0.69      1310\n",
      "   macro avg       0.66      0.65      0.66      1310\n",
      "weighted avg       0.69      0.69      0.69      1310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[669 181]\n",
      " [222 238]]\n",
      "Accuracy: 0.6923664122137405\n",
      "Modelo guardado como best_stacking_model_3_PRUEBA.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "# Cargar los modelos guardados\n",
    "best_forest_stack = load('best_forest_model_random_3_PRUEBA.joblib')\n",
    "best_xgb_stack = load('best_xgb_model_random_3_PRUEBA.joblib')\n",
    "best_baggin_stack = load('best_bagging_model_random_3_PRUEBA.joblib')\n",
    "best_lgbm_stack = load('best_lgbm_model_random_3_PRUEBA.joblib')\n",
    "best_cat_stack = load('best_catboost_model_random_3_PRUEBA.joblib')\n",
    "best_log_stack = load('best_log_model_3_PRUEBA.joblib')\n",
    "best_gb_stack = load('best_gb_model_random_3_PRUEBA.joblib')\n",
    "best_knn_stack = load(\"best_knn_model_random_3_PRUEBA.joblib\")\n",
    "best_svm_stack = load(\"best_svm_model_random_3_PRUEBA.joblib\")\n",
    "best_tree_stack = load(\"best_tree_model_3_PRUEBA.joblib\")\n",
    "# Definir el meta-modelo\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Crear el StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('forest', best_forest_stack),\n",
    "        ('xgb', best_xgb_stack),\n",
    "        ('baggin', best_baggin_stack),\n",
    "        ('lgbm', best_lgbm_stack),\n",
    "        ('cat', best_cat_stack),\n",
    "        ('log', best_log_stack),\n",
    "        ('gb', best_gb_stack),\n",
    "        ('knn', best_knn_stack),\n",
    "        ('svm', best_svm_stack),\n",
    "        ('tree', best_tree_stack),\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenar el meta-modelo\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el meta-modelo\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_stack))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
    "\n",
    "# Guardar el meta-modelo\n",
    "model_filename = 'best_stacking_model_3_PRUEBA.joblib'\n",
    "dump(stacking_clf, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking con Meta modelo RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
