{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PREPARACION DE DATOS PARA ENTRENAR MODELO**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/machinelearning.jpg\" alt=\"Texto alternativo\" width=\"2100\" height=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el data, anteriormente lo pasamos todo a numericas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resultados = pd.read_csv(\"../BASESDEDATOS/CSVs/LimpiezaEncoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resultado\n",
       "1    0.472937\n",
       "2    0.276119\n",
       "0    0.250944\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resultados.Resultado.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Como vemos y como vimos en las gráficas en el EDA la distribución de nuestro target \"Resultado\" es desigual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vamos aplicar un SMOTE para igualar las clases**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/smote.jpg\" alt=\"Texto alternativo\" width=\"1800\" height=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y la variable objetivo\n",
    "X = data_resultados.drop(columns=['Resultado'])\n",
    "y = data_resultados['Resultado']\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resultado\n",
       "1    0.333333\n",
       "0    0.333333\n",
       "2    0.333333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resampled = pd.concat([X_resampled, y_resampled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resultado                               1.000000\n",
       "Rendimiento_Ranking_Visitante           0.151644\n",
       "Rendimiento_Vistante                    0.135113\n",
       "Ranking_Visitante_Cuadrado              0.134932\n",
       "Diferencia_Ranking                      0.131193\n",
       "Diferencia_Puntos_Visitante             0.131039\n",
       "Diferencia_Puntos_Local                 0.131039\n",
       "Ranking_Visitante                       0.126011\n",
       "Ratio_Jornada_Visitante                 0.118895\n",
       "Posicion_Visitante                      0.117323\n",
       "Ratio_Goles_por_partido_Visitante       0.113894\n",
       "Diferencia_Posicion                     0.108327\n",
       "Local_Es_Favorito                       0.107054\n",
       "Visitante_Es_Favorito                   0.106274\n",
       "Goles_Marcados_Visitante_Acumulados     0.093092\n",
       "Estado_Tabla_Visitante                  0.091495\n",
       "Local_Es_Ofensivo                       0.090371\n",
       "Puntos_Acumulados_Visitantes            0.085018\n",
       "Goles_Acumulados_Visitantes             0.083298\n",
       "Media_Goles_Visitante                   0.077370\n",
       "Visitante_Es_Defensivo                  0.074975\n",
       "Visitante_Es_Ofensivo                   0.065388\n",
       "Ranking_Local                           0.062329\n",
       "Ranking_Local_Cuadrado                  0.060975\n",
       "Rendimiento_Ranking_Local               0.054924\n",
       "Rendimiento_Local                       0.052142\n",
       "Local_Es_Defensivo                      0.044814\n",
       "Media_Goles_Local                       0.042813\n",
       "Media_Goles                             0.042062\n",
       "Rendimiento_Fuera_Visitante             0.040310\n",
       "Posicion_Local                          0.035840\n",
       "Ratio_Goles_por_partido_Local           0.033639\n",
       "Ratio_Jornada_Local                     0.032762\n",
       "Goles_Encajados_Visitante_Acumulados    0.031941\n",
       "Goles_Encajados_Local_Acumulados        0.026149\n",
       "Racha_Puntos_Visitante                  0.023635\n",
       "Estado_Tabla_Local                      0.023551\n",
       "Visitante                               0.022865\n",
       "Racha_Puntos_Local                      0.020501\n",
       "Temporada                               0.020262\n",
       "Porterias_Cero_Visitante_Acum           0.016462\n",
       "Racha_Visitante                         0.016400\n",
       "Puntos_Acumulados_Local                 0.013049\n",
       "Jornada                                 0.012504\n",
       "Local                                   0.011554\n",
       "Goles_Acumulados_Local                  0.011000\n",
       "Racha_Local                             0.008243\n",
       "Forma_Reciente_Local                    0.007628\n",
       "Porterias_Cero_Local_Acum               0.005766\n",
       "Goles_Marcados_Local_Acumulados         0.005191\n",
       "Rendimiento_Casa_Local                  0.003628\n",
       "Forma_Reciente_Visitante                0.000158\n",
       "Name: Resultado, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular la correlación de todas las columnas con 'Resultado'\n",
    "target_column = 'Resultado'\n",
    "correlation_with_target = abs(data_resampled.corr()[target_column])\n",
    "\n",
    "# Ordenar las correlaciones de mayor a menor\n",
    "correlation_with_target = correlation_with_target.sort_values(ascending=False)\n",
    "\n",
    "# Mostrar las correlaciones  #Algo de correlación hemos aumentado no mucho pero algo mejor.\n",
    "correlation_with_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Separación X_Train e Y_Train**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/xtrain.jpg\" alt=\"Texto alternativo\" width=\"1400\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Construcción de los modelos**\n",
    "\n",
    "\n",
    "<img src=\"../Imagenes/construccion.jpg\" alt=\"Texto alternativo\" width=\"1600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 1: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Mejores parámetros: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.35      0.39       812\n",
      "           1       0.50      0.64      0.56       817\n",
      "           2       0.50      0.47      0.48       851\n",
      "\n",
      "    accuracy                           0.49      2480\n",
      "   macro avg       0.48      0.49      0.48      2480\n",
      "weighted avg       0.48      0.49      0.48      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[283 283 246]\n",
      " [139 523 155]\n",
      " [211 240 400]]\n",
      "Accuracy: 0.48629032258064514\n",
      "Modelo guardado como best_log_model_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el modelo y los nuevos parámetros de GridSearch\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "param_grid = [\n",
    "    {'C': [100], 'solver': ['liblinear'], 'penalty': ['l1', 'l2']},\n",
    "    {'C': [100], 'solver': ['saga'], 'penalty': ['l1', 'l2'], 'l1_ratio': [0, 0.5, 1]}\n",
    "]\n",
    "\n",
    "# Aplicar GridSearchCV\n",
    "grid_log_reg = GridSearchCV(log_reg, param_grid, cv=3, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_log_reg = grid_log_reg.best_estimator_\n",
    "y_pred_log_reg = best_log_reg.predict(X_test)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Mejores parámetros:\", grid_log_reg.best_params_)\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_log_model_2.joblib'\n",
    "dump(best_log_reg, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 2: Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "Mejores parámetros: {'max_depth': 12, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.49      0.47       812\n",
      "           1       0.47      0.50      0.48       817\n",
      "           2       0.50      0.43      0.46       851\n",
      "\n",
      "    accuracy                           0.47      2480\n",
      "   macro avg       0.47      0.47      0.47      2480\n",
      "weighted avg       0.47      0.47      0.47      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[400 236 176]\n",
      " [222 407 188]\n",
      " [263 223 365]]\n",
      "Accuracy: 0.47258064516129034\n",
      "Modelo guardado como best_tree_model_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Definir el modelo y los nuevos parámetros de GridSearch\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 8, 10, 12, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Aplicar GridSearchCV\n",
    "grid_tree = GridSearchCV(decision_tree, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_tree = grid_tree.best_estimator_\n",
    "y_pred_tree = best_tree.predict(X_test)\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(\"Mejores parámetros:\", grid_tree.best_params_)\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_tree_model_2.joblib'\n",
    "dump(best_tree, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 3: Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.65       812\n",
      "           1       0.59      0.64      0.61       817\n",
      "           2       0.67      0.66      0.66       851\n",
      "\n",
      "    accuracy                           0.64      2480\n",
      "   macro avg       0.64      0.64      0.64      2480\n",
      "weighted avg       0.64      0.64      0.64      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[507 181 124]\n",
      " [139 520 158]\n",
      " [112 177 562]]\n",
      "Accuracy: 0.6407258064516129\n",
      "Modelo guardado como best_forest_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300, 400, 500, 600],\n",
    "    'max_depth': [30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(random_forest, param_distributions, n_iter=150, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_forest_random = random_search.best_estimator_\n",
    "y_pred_forest_random = best_forest_random.predict(X_test)\n",
    "print(\"Random Forest Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_forest_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_forest_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_forest_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_forest_model_random_2.joblib'\n",
    "dump(best_forest_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 4: Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 11, 'learning_rate': 0.2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.61      0.64       812\n",
      "           1       0.60      0.67      0.63       817\n",
      "           2       0.67      0.66      0.66       851\n",
      "\n",
      "    accuracy                           0.64      2480\n",
      "   macro avg       0.65      0.64      0.64      2480\n",
      "weighted avg       0.65      0.64      0.64      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[495 185 132]\n",
      " [128 544 145]\n",
      " [118 175 558]]\n",
      "Accuracy: 0.6439516129032258\n",
      "Modelo guardado como best_gb_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [400,500,600],\n",
    "    'learning_rate': [0.2, 0.3,0.35,0.4],\n",
    "    'max_depth': [5,7,9,11],\n",
    "    'min_samples_split': [2, 5,7],\n",
    "    'min_samples_leaf': [6, 8,10,12],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_gb = RandomizedSearchCV(gradient_boosting, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_gb_random = random_search_gb.best_estimator_\n",
    "y_pred_gb_random = best_gb_random.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_gb.best_params_)\n",
    "print(classification_report(y_test, y_pred_gb_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_gb_model_random_2.joblib'\n",
    "dump(best_gb_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 5: Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) con RandomizedSearchCV\n",
      "Mejores parámetros: {'kernel': 'rbf', 'gamma': 'scale', 'degree': 2, 'C': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61       812\n",
      "           1       0.55      0.47      0.51       817\n",
      "           2       0.62      0.61      0.61       851\n",
      "\n",
      "    accuracy                           0.58      2480\n",
      "   macro avg       0.58      0.58      0.58      2480\n",
      "weighted avg       0.58      0.58      0.58      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[536 142 134]\n",
      " [244 387 186]\n",
      " [162 172 517]]\n",
      "Accuracy: 0.5806451612903226\n",
      "Modelo guardado como best_svm_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "svm_model = SVC(random_state=42)\n",
    "param_distributions = {\n",
    "    'C': [10, 100,200,300],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]  # Solo aplica si el kernel es 'poly'\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_svm = RandomizedSearchCV(svm_model, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_svm_random = random_search_svm.best_estimator_\n",
    "y_pred_svm_random = best_svm_random.predict(X_test)\n",
    "print(\"Support Vector Machine (SVM) con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_svm.best_params_)\n",
    "print(classification_report(y_test, y_pred_svm_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_svm_model_random_2.joblib'\n",
    "dump(best_svm_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 6: K-Nearest Neighbors (KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN) con RandomizedSearchCV\n",
      "Mejores parámetros: {'weights': 'distance', 'n_neighbors': 3, 'metric': 'manhattan', 'leaf_size': 60, 'algorithm': 'brute'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66       812\n",
      "           1       0.64      0.47      0.54       817\n",
      "           2       0.63      0.70      0.66       851\n",
      "\n",
      "    accuracy                           0.63      2480\n",
      "   macro avg       0.63      0.63      0.62      2480\n",
      "weighted avg       0.63      0.63      0.62      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[579  99 134]\n",
      " [223 384 210]\n",
      " [137 119 595]]\n",
      "Accuracy: 0.6282258064516129\n",
      "Modelo guardado como best_knn_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "knn_model = KNeighborsClassifier()\n",
    "param_distributions = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40, 50,60]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_knn = RandomizedSearchCV(knn_model, param_distributions, n_iter=300, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_knn_random = random_search_knn.best_estimator_\n",
    "y_pred_knn_random = best_knn_random.predict(X_test)\n",
    "print(\"K-Nearest Neighbors (KNN) con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_knn.best_params_)\n",
    "print(classification_report(y_test, y_pred_knn_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_knn_model_random_2.joblib'\n",
    "dump(best_knn_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 7: Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes con GridSearchCV\n",
      "Mejores parámetros: {'var_smoothing': 0.002848035868435802}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.36      0.37       812\n",
      "           1       0.49      0.49      0.49       817\n",
      "           2       0.48      0.50      0.49       851\n",
      "\n",
      "    accuracy                           0.45      2480\n",
      "   macro avg       0.45      0.45      0.45      2480\n",
      "weighted avg       0.45      0.45      0.45      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[296 240 276]\n",
      " [232 398 187]\n",
      " [252 172 427]]\n",
      "Accuracy: 0.4520161290322581\n",
      "Modelo guardado como best_naive_bayes_model_grid_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Definir el modelo y los parámetros de GridSearch\n",
    "naive_bayes = GaussianNB()\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "# Aplicar GridSearchCV\n",
    "grid_search_nb = GridSearchCV(naive_bayes, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_nb_grid = grid_search_nb.best_estimator_\n",
    "y_pred_nb_grid = best_nb_grid.predict(X_test)\n",
    "print(\"Naive Bayes con GridSearchCV\")\n",
    "print(\"Mejores parámetros:\", grid_search_nb.best_params_)\n",
    "print(classification_report(y_test, y_pred_nb_grid))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nb_grid))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb_grid))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_naive_bayes_model_grid_2.joblib'\n",
    "dump(best_nb_grid, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 8: XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost con RandomizedSearchCV\n",
      "Mejores parámetros: {'subsample': 0.8, 'n_estimators': 400, 'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64       812\n",
      "           1       0.58      0.65      0.62       817\n",
      "           2       0.67      0.64      0.66       851\n",
      "\n",
      "    accuracy                           0.64      2480\n",
      "   macro avg       0.64      0.64      0.64      2480\n",
      "weighted avg       0.64      0.64      0.64      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[501 193 118]\n",
      " [136 533 148]\n",
      " [119 186 546]]\n",
      "Accuracy: 0.6370967741935484\n",
      "Modelo guardado como best_xgb_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "param_distributions = {\n",
    "    'n_estimators': [300, 400, 500, 600],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [ 7, 9, 11, 13, 15],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.125, 0.2]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_xgb_random = random_search_xgb.best_estimator_\n",
    "y_pred_xgb_random = best_xgb_random.predict(X_test)\n",
    "print(\"XGBoost con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_xgb.best_params_)\n",
    "print(classification_report(y_test, y_pred_xgb_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_xgb_model_random_2.joblib'\n",
    "dump(best_xgb_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 9: Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'n_estimators': 300, 'max_samples': 1.0, 'max_features': 0.4, 'bootstrap_features': False, 'bootstrap': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       812\n",
      "           1       0.61      0.64      0.62       817\n",
      "           2       0.67      0.67      0.67       851\n",
      "\n",
      "    accuracy                           0.65      2480\n",
      "   macro avg       0.65      0.65      0.65      2480\n",
      "weighted avg       0.65      0.65      0.65      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[516 170 126]\n",
      " [135 523 159]\n",
      " [107 170 574]]\n",
      "Accuracy: 0.6504032258064516\n",
      "Modelo guardado como best_bagging_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier( random_state=42)\n",
    "\n",
    "# Definir los parámetros para RandomizedSearch\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200,300,400],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.4, 0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'bootstrap_features': [True, False]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_bagging = RandomizedSearchCV(bagging_model, param_distributions, n_iter=100, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_bagging_random = random_search_bagging.best_estimator_\n",
    "y_pred_bagging_random = best_bagging_random.predict(X_test)\n",
    "print(\"Bagging Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_bagging.best_params_)\n",
    "print(classification_report(y_test, y_pred_bagging_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_bagging_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bagging_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_bagging_model_random_2.joblib'\n",
    "dump(best_bagging_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 10: LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6964\n",
      "[LightGBM] [Info] Number of data points in the train set: 9919, number of used features: 51\n",
      "[LightGBM] [Info] Start training from score -1.094186\n",
      "[LightGBM] [Info] Start training from score -1.095693\n",
      "[LightGBM] [Info] Start training from score -1.105999\n",
      "LightGBM Classifier con RandomizedSearchCV\n",
      "Mejores parámetros: {'objective': 'multiclass', 'num_leaves': 90, 'n_estimators': 500, 'max_depth': 40, 'learning_rate': 0.1, 'boosting_type': 'gbdt'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       812\n",
      "           1       0.59      0.67      0.63       817\n",
      "           2       0.68      0.65      0.67       851\n",
      "\n",
      "    accuracy                           0.65      2480\n",
      "   macro avg       0.65      0.65      0.65      2480\n",
      "weighted avg       0.65      0.65      0.65      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[503 192 117]\n",
      " [128 545 144]\n",
      " [114 180 557]]\n",
      "Accuracy: 0.6471774193548387\n",
      "Modelo guardado como best_lgbm_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Definir el modelo y los parámetros de RandomizedSearch\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "param_distributions = {\n",
    "    'n_estimators': [400,500,600],\n",
    "    'learning_rate': [ 0.1, 0.2, 0.3],\n",
    "    'num_leaves': [31, 50, 70, 90],\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'objective': ['multiclass'],\n",
    "    'max_depth': [-1, 10, 20, 30 , 40]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm_model, param_distributions, n_iter=100, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "y_pred_lgbm = best_lgbm.predict(X_test)\n",
    "print(\"LightGBM Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_lgbm.best_params_)\n",
    "print(classification_report(y_test, y_pred_lgbm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lgbm))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_lgbm_model_random_2.joblib'\n",
    "dump(best_lgbm, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo 11: CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Definir el modelo de CatBoost\n",
    "catboost_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Definir los parámetros para RandomizedSearch\n",
    "param_distributions = {\n",
    "    'iterations': [200, 500, 600, 700],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'depth': [10, 12, 14, 16],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'border_count': [100, 150, 200]\n",
    "}\n",
    "\n",
    "# Aplicar RandomizedSearchCV\n",
    "random_search_catboost = RandomizedSearchCV(catboost_model, param_distributions, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el mejor modelo\n",
    "best_catboost_random = random_search_catboost.best_estimator_\n",
    "y_pred_catboost_random = best_catboost_random.predict(X_test)\n",
    "print(\"CatBoost Classifier con RandomizedSearchCV\")\n",
    "print(\"Mejores parámetros:\", random_search_catboost.best_params_)\n",
    "print(classification_report(y_test, y_pred_catboost_random))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_catboost_random))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_catboost_random))\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = 'best_catboost_model_random_2.joblib'\n",
    "dump(best_catboost_random, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking con Meta modelo LogisticRegresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       812\n",
      "           1       0.61      0.65      0.63       817\n",
      "           2       0.69      0.68      0.68       851\n",
      "\n",
      "    accuracy                           0.66      2480\n",
      "   macro avg       0.66      0.66      0.66      2480\n",
      "weighted avg       0.66      0.66      0.66      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[537 165 110]\n",
      " [144 529 144]\n",
      " [109 167 575]]\n",
      "Accuracy: 0.6616935483870968\n",
      "Modelo guardado como best_stacking_model_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "# Cargar los modelos guardados\n",
    "best_forest_stack = load('best_forest_model_random_2.joblib')\n",
    "best_gb_stack = load('best_gb_model_random_2.joblib')\n",
    "best_knn_stack = load('best_knn_model_random_2.joblib')\n",
    "best_xgb_stack = load('best_xgb_model_random_2.joblib')\n",
    "best_baggin_stack = load('best_bagging_model_random_2.joblib')\n",
    "best_lgbm_stack = load('best_lgbm_model_random_2.joblib')\n",
    "best_cat_stack = load('best_catboost_model_random.joblib')\n",
    "\n",
    "# Definir el meta-modelo\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Crear el StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('forest', best_forest_stack),\n",
    "        ('gb', best_gb_stack),\n",
    "        ('knn', best_knn_stack),\n",
    "        ('xgb', best_xgb_stack),\n",
    "        ('baggin', best_baggin_stack),\n",
    "        ('lgbm', best_lgbm_stack),\n",
    "        ('cat', best_cat_stack),\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenar el meta-modelo\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el meta-modelo\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_stack))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
    "\n",
    "# Guardar el meta-modelo\n",
    "model_filename = 'best_stacking_model_2.joblib'\n",
    "dump(stacking_clf, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       812\n",
      "           1       0.61      0.65      0.63       817\n",
      "           2       0.69      0.68      0.68       851\n",
      "\n",
      "    accuracy                           0.66      2480\n",
      "   macro avg       0.66      0.66      0.66      2480\n",
      "weighted avg       0.66      0.66      0.66      2480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[537 165 110]\n",
      " [144 529 144]\n",
      " [109 167 575]]\n",
      "Accuracy: 0.6616935483870968\n",
      "Modelo guardado como best_stacking_model_random_2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Cargar los modelos guardados\n",
    "best_forest_stack = load('best_forest_model_random_2.joblib')\n",
    "best_gb_stack = load('best_gb_model_random_2.joblib')\n",
    "best_knn_stack = load('best_knn_model_random_2.joblib')\n",
    "best_xgb_stack = load('best_xgb_model_random_2.joblib')\n",
    "best_baggin_stack = load('best_bagging_model_random_2.joblib')\n",
    "best_lgbm_stack = load('best_lgbm_model_random_2.joblib')\n",
    "best_cat_stack = load('best_catboost_model_random.joblib')\n",
    "\n",
    "# Definir el meta-modelo\n",
    "meta_model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Crear el StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('forest', best_forest_stack),\n",
    "        ('gb', best_gb_stack),\n",
    "        ('knn', best_knn_stack),\n",
    "        ('xgb', best_xgb_stack),\n",
    "        ('baggin', best_baggin_stack),\n",
    "        ('lgbm', best_lgbm_stack),\n",
    "        ('cat', best_cat_stack),\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenar el meta-modelo\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el meta-modelo\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_stack))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
    "\n",
    "# Guardar el meta-modelo\n",
    "model_filename = 'best_stacking_model_random_2.joblib'\n",
    "dump(stacking_clf, model_filename)\n",
    "print(f\"Modelo guardado como {model_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
